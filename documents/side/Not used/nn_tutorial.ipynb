{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-114.,  -41., -170.,  -49., -113.,  -39., -170.,  -47., -115.,\n          -39., -169.,  -45., -115.,   -1.,  -91.,   -1.,  -90.,  -14.,\n          -42.,  -10.,  -45.,    1.,    3.,    0.,   -1.,    7.,   11.,\n            7.,    6.,  -11.,   16.,  -12.,    9.]]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing somethingg\n",
    "import torch\n",
    "import numpy as np\n",
    "lmlist = [[0, 271, 139], [1, 271, 135], [2, 271, 134], [3, 272, 134], [4, 271, 135], [5, 271, 135], [6, 271, 135], [7, 278, 131], [8, 275, 134], [9, 275, 141], [10, 275, 141], [11, 292, 144], [12, 292, 151], [13, 270, 146], [14, 276, 173], [15, 263, 134], [16, 258, 187], [17, 260, 131], [18, 252, 188], [19, 262, 131], [20, 254, 186], [21, 262, 132], [22, 256, 186], [23, 300, 210], [24, 300, 211], [25, 287, 259], [26, 291, 256], [27, 302, 304], [28, 301, 300], [29, 308, 312], [30, 308, 307], [31, 290, 317], [32, 289, 310]]\n",
    "\n",
    "a = np.array(lmlist)[:, 1:].reshape(1, 1, 2, 33).astype(float)\n",
    "\n",
    "a[:,:,1, :] - a[:, :, 1, 23]\n",
    "# b = torch.from_numpy(a)\n",
    "# b.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is `torch.nn` *really*?\n",
    "============================\n",
    "by Jeremy Howard, `fast.ai <https://www.fast.ai>`_. Thanks to Rachel Thomas and Francisco Ingham.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,\n",
    "click the link at the top of the page.\n",
    "\n",
    "PyTorch provides the elegantly designed modules and classes `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ ,\n",
    "`torch.optim <https://pytorch.org/docs/stable/optim.html>`_ ,\n",
    "`Dataset <https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`_ ,\n",
    "and `DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`_\n",
    "to help you create and train neural networks.\n",
    "In order to fully utilize their power and customize\n",
    "them for your problem, you need to really understand exactly what they're\n",
    "doing. To develop this understanding, we will first train basic neural net\n",
    "on the MNIST data set without using any features from these models; we will\n",
    "initially only use the most basic PyTorch tensor functionality. Then, we will\n",
    "incrementally add one feature from ``torch.nn``, ``torch.optim``, ``Dataset``, or\n",
    "``DataLoader`` at a time, showing exactly what each piece does, and how it\n",
    "works to make the code either more concise, or more flexible.\n",
    "\n",
    "**This tutorial assumes you already have PyTorch installed, and are familiar\n",
    "with the basics of tensor operations.** (If you're familiar with Numpy array\n",
    "operations, you'll find the PyTorch tensor operations used here nearly identical).\n",
    "\n",
    "MNIST data setup\n",
    "----------------\n",
    "\n",
    "We will use the classic `MNIST <http://deeplearning.net/data/mnist/>`_ dataset,\n",
    "which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
    "\n",
    "We will use `pathlib <https://docs.python.org/3/library/pathlib.html>`_\n",
    "for dealing with paths (part of the Python 3 standard library), and will\n",
    "download the dataset using\n",
    "`requests <http://docs.python-requests.org/en/master/>`_. We will only\n",
    "import modules when we use them, so you can see exactly what's being\n",
    "used at each point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is in numpy array format, and has been stored using pickle,\n",
    "a python-specific format for serializing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 784)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 x 28, and is being stored as a flattened row of length\n",
    "784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n",
    "first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n",
    "convert our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([50000, 784])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net from scratch (no torch.nn)\n",
    "---------------------------------------------\n",
    "\n",
    "Let's first create a model using nothing but PyTorch tensor operations. We're assuming\n",
    "you're already familiar with the basics of neural networks. (If you're not, you can\n",
    "learn them at `course.fast.ai <https://course.fast.ai>`_).\n",
    "\n",
    "PyTorch provides methods to create random or zero-filled tensors, which we will\n",
    "use to create our weights and bias for a simple linear model. These are just regular\n",
    "tensors, with one very special addition: we tell PyTorch that they require a\n",
    "gradient. This causes PyTorch to record all of the operations done on the tensor,\n",
    "so that it can calculate the gradient during back-propagation *automatically*!\n",
    "\n",
    "For the weights, we set ``requires_grad`` **after** the initialization, since we\n",
    "don't want that step included in the gradient. (Note that a trailing ``_`` in\n",
    "PyTorch signifies that the operation is performed in-place.)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>We are initializing the weights here with\n",
    "   `Xavier initialisation <http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>`_\n",
    "   (by multiplying with 1/sqrt(n)).</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to PyTorch's ability to calculate gradients automatically, we can\n",
    "use any standard Python function (or callable object) as a model! So\n",
    "let's just write a plain matrix multiplication and broadcasted addition\n",
    "to create a simple linear model. We also need an activation function, so\n",
    "we'll write `log_softmax` and use it. Remember: although PyTorch\n",
    "provides lots of pre-written loss functions, activation functions, and\n",
    "so forth, you can easily write your own using plain python. PyTorch will\n",
    "even create fast GPU or vectorized CPU code for your function\n",
    "automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the ``@`` stands for the matrix multiplication operation. We will call\n",
    "our function on one batch of data (in this case, 64 images).  This is\n",
    "one *forward pass*.  Note that our predictions won't be any better than\n",
    "random at this stage, since we start with random weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8789, -2.3620, -1.9477, -2.4152, -2.2098, -2.3829, -2.5459, -2.5049,\n",
      "        -2.4204, -2.6583], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the ``preds`` tensor contains not only the tensor values, but also a\n",
    "gradient function. We'll use this later to do backprop.\n",
    "\n",
    "Let's implement negative log-likelihood to use as the loss function\n",
    "(again, we can just use standard Python):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our loss with our random model, so we can see if we improve\n",
    "after a backprop pass later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3655, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also implement a function to calculate the accuracy of our model.\n",
    "For each prediction, if the index with the largest value matches the\n",
    "target value, then the prediction was correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy of our random model, so we can see if our\n",
    "accuracy improves as our loss improves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a training loop.  For each iteration, we will:\n",
    "\n",
    "- select a mini-batch of data (of size ``bs``)\n",
    "- use the model to make predictions\n",
    "- calculate the loss\n",
    "- ``loss.backward()`` updates the gradients of the model, in this case, ``weights``\n",
    "  and ``bias``.\n",
    "\n",
    "We now use these gradients to update the weights and bias.  We do this\n",
    "within the ``torch.no_grad()`` context manager, because we do not want these\n",
    "actions to be recorded for our next calculation of the gradient.  You can read\n",
    "more about how PyTorch's Autograd records operations\n",
    "`here <https://pytorch.org/docs/stable/notes/autograd.html>`_.\n",
    "\n",
    "We then set the\n",
    "gradients to zero, so that we are ready for the next loop.\n",
    "Otherwise, our gradients would record a running tally of all the operations\n",
    "that had happened (i.e. ``loss.backward()`` *adds* the gradients to whatever is\n",
    "already stored, rather than replacing them).\n",
    "\n",
    ".. tip:: You can use the standard python debugger to step through PyTorch\n",
    "   code, allowing you to check the various variable values at each step.\n",
    "   Uncomment ``set_trace()`` below to try it out.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it: we've created and trained a minimal neural network (in this case, a\n",
    "logistic regression, since we have no hidden layers) entirely from scratch!\n",
    "\n",
    "Let's check the loss and accuracy and compare those to what we got\n",
    "earlier. We expect that the loss will have decreased and accuracy to\n",
    "have increased, and they have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0814, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch.nn.functional\n",
    "------------------------------\n",
    "\n",
    "We will now refactor our code, so that it does the same thing as before, only\n",
    "we'll start taking advantage of PyTorch's ``nn`` classes to make it more concise\n",
    "and flexible. At each step from here, we should be making our code one or more\n",
    "of: shorter, more understandable, and/or more flexible.\n",
    "\n",
    "The first and easiest step is to make our code shorter by replacing our\n",
    "hand-written activation and loss functions with those from ``torch.nn.functional``\n",
    "(which is generally imported into the namespace ``F`` by convention). This module\n",
    "contains all the functions in the ``torch.nn`` library (whereas other parts of the\n",
    "library contain classes). As well as a wide range of loss and activation\n",
    "functions, you'll also find here some convenient functions for creating neural\n",
    "nets, such as pooling functions. (There are also functions for doing convolutions,\n",
    "linear layers, etc, but as we'll see, these are usually better handled using\n",
    "other parts of the library.)\n",
    "\n",
    "If you're using negative log likelihood loss and log softmax activation,\n",
    "then Pytorch provides a single function ``F.cross_entropy`` that combines\n",
    "the two. So we can even remove the activation function from our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\n",
    "confirm that our loss and accuracy are the same as before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0814, grad_fn=<NllLossBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using nn.Module\n",
    "-----------------------------\n",
    "Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more\n",
    "concise training loop. We subclass ``nn.Module`` (which itself is a class and\n",
    "able to keep track of state).  In this case, we want to create a class that\n",
    "holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n",
    "number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``)\n",
    "which we will be using.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a\n",
    "   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python\n",
    "   concept of a (lowercase ``m``) `module <https://docs.python.org/3/tutorial/modules.html>`_,\n",
    "   which is a file of Python code that can be imported.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now using an object instead of just using a function, we\n",
    "first have to instantiate our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the loss in the same way as before. Note that\n",
    "``nn.Module`` objects are used as if they are functions (i.e they are\n",
    "*callable*), but behind the scenes Pytorch will call our ``forward``\n",
    "method automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2992, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously for our training loop we had to update the values for each parameter\n",
    "by name, and manually zero out the grads for each parameter separately, like this:\n",
    "::\n",
    "  with torch.no_grad():\n",
    "      weights -= weights.grad * lr\n",
    "      bias -= bias.grad * lr\n",
    "      weights.grad.zero_()\n",
    "      bias.grad.zero_()\n",
    "\n",
    "\n",
    "Now we can take advantage of model.parameters() and model.zero_grad() (which\n",
    "are both defined by PyTorch for ``nn.Module``) to make those steps more concise\n",
    "and less prone to the error of forgetting some of our parameters, particularly\n",
    "if we had a more complicated model:\n",
    "::\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters(): p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "\n",
    "\n",
    "We'll wrap our little training loop in a ``fit`` function so we can run it\n",
    "again later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 64\n",
      "64 128\n",
      "128 192\n",
      "192 256\n",
      "256 320\n",
      "320 384\n",
      "384 448\n",
      "448 512\n",
      "512 576\n",
      "576 640\n",
      "640 704\n",
      "704 768\n",
      "768 832\n",
      "832 896\n",
      "896 960\n",
      "960 1024\n",
      "1024 1088\n",
      "1088 1152\n",
      "1152 1216\n",
      "1216 1280\n",
      "1280 1344\n",
      "1344 1408\n",
      "1408 1472\n",
      "1472 1536\n",
      "1536 1600\n",
      "1600 1664\n",
      "1664 1728\n",
      "1728 1792\n",
      "1792 1856\n",
      "1856 1920\n",
      "1920 1984\n",
      "1984 2048\n",
      "2048 2112\n",
      "2112 2176\n",
      "2176 2240\n",
      "2240 2304\n",
      "2304 2368\n",
      "2368 2432\n",
      "2432 2496\n",
      "2496 2560\n",
      "2560 2624\n",
      "2624 2688\n",
      "2688 2752\n",
      "2752 2816\n",
      "2816 2880\n",
      "2880 2944\n",
      "2944 3008\n",
      "3008 3072\n",
      "3072 3136\n",
      "3136 3200\n",
      "3200 3264\n",
      "3264 3328\n",
      "3328 3392\n",
      "3392 3456\n",
      "3456 3520\n",
      "3520 3584\n",
      "3584 3648\n",
      "3648 3712\n",
      "3712 3776\n",
      "3776 3840\n",
      "3840 3904\n",
      "3904 3968\n",
      "3968 4032\n",
      "4032 4096\n",
      "4096 4160\n",
      "4160 4224\n",
      "4224 4288\n",
      "4288 4352\n",
      "4352 4416\n",
      "4416 4480\n",
      "4480 4544\n",
      "4544 4608\n",
      "4608 4672\n",
      "4672 4736\n",
      "4736 4800\n",
      "4800 4864\n",
      "4864 4928\n",
      "4928 4992\n",
      "4992 5056\n",
      "5056 5120\n",
      "5120 5184\n",
      "5184 5248\n",
      "5248 5312\n",
      "5312 5376\n",
      "5376 5440\n",
      "5440 5504\n",
      "5504 5568\n",
      "5568 5632\n",
      "5632 5696\n",
      "5696 5760\n",
      "5760 5824\n",
      "5824 5888\n",
      "5888 5952\n",
      "5952 6016\n",
      "6016 6080\n",
      "6080 6144\n",
      "6144 6208\n",
      "6208 6272\n",
      "6272 6336\n",
      "6336 6400\n",
      "6400 6464\n",
      "6464 6528\n",
      "6528 6592\n",
      "6592 6656\n",
      "6656 6720\n",
      "6720 6784\n",
      "6784 6848\n",
      "6848 6912\n",
      "6912 6976\n",
      "6976 7040\n",
      "7040 7104\n",
      "7104 7168\n",
      "7168 7232\n",
      "7232 7296\n",
      "7296 7360\n",
      "7360 7424\n",
      "7424 7488\n",
      "7488 7552\n",
      "7552 7616\n",
      "7616 7680\n",
      "7680 7744\n",
      "7744 7808\n",
      "7808 7872\n",
      "7872 7936\n",
      "7936 8000\n",
      "8000 8064\n",
      "8064 8128\n",
      "8128 8192\n",
      "8192 8256\n",
      "8256 8320\n",
      "8320 8384\n",
      "8384 8448\n",
      "8448 8512\n",
      "8512 8576\n",
      "8576 8640\n",
      "8640 8704\n",
      "8704 8768\n",
      "8768 8832\n",
      "8832 8896\n",
      "8896 8960\n",
      "8960 9024\n",
      "9024 9088\n",
      "9088 9152\n",
      "9152 9216\n",
      "9216 9280\n",
      "9280 9344\n",
      "9344 9408\n",
      "9408 9472\n",
      "9472 9536\n",
      "9536 9600\n",
      "9600 9664\n",
      "9664 9728\n",
      "9728 9792\n",
      "9792 9856\n",
      "9856 9920\n",
      "9920 9984\n",
      "9984 10048\n",
      "10048 10112\n",
      "10112 10176\n",
      "10176 10240\n",
      "10240 10304\n",
      "10304 10368\n",
      "10368 10432\n",
      "10432 10496\n",
      "10496 10560\n",
      "10560 10624\n",
      "10624 10688\n",
      "10688 10752\n",
      "10752 10816\n",
      "10816 10880\n",
      "10880 10944\n",
      "10944 11008\n",
      "11008 11072\n",
      "11072 11136\n",
      "11136 11200\n",
      "11200 11264\n",
      "11264 11328\n",
      "11328 11392\n",
      "11392 11456\n",
      "11456 11520\n",
      "11520 11584\n",
      "11584 11648\n",
      "11648 11712\n",
      "11712 11776\n",
      "11776 11840\n",
      "11840 11904\n",
      "11904 11968\n",
      "11968 12032\n",
      "12032 12096\n",
      "12096 12160\n",
      "12160 12224\n",
      "12224 12288\n",
      "12288 12352\n",
      "12352 12416\n",
      "12416 12480\n",
      "12480 12544\n",
      "12544 12608\n",
      "12608 12672\n",
      "12672 12736\n",
      "12736 12800\n",
      "12800 12864\n",
      "12864 12928\n",
      "12928 12992\n",
      "12992 13056\n",
      "13056 13120\n",
      "13120 13184\n",
      "13184 13248\n",
      "13248 13312\n",
      "13312 13376\n",
      "13376 13440\n",
      "13440 13504\n",
      "13504 13568\n",
      "13568 13632\n",
      "13632 13696\n",
      "13696 13760\n",
      "13760 13824\n",
      "13824 13888\n",
      "13888 13952\n",
      "13952 14016\n",
      "14016 14080\n",
      "14080 14144\n",
      "14144 14208\n",
      "14208 14272\n",
      "14272 14336\n",
      "14336 14400\n",
      "14400 14464\n",
      "14464 14528\n",
      "14528 14592\n",
      "14592 14656\n",
      "14656 14720\n",
      "14720 14784\n",
      "14784 14848\n",
      "14848 14912\n",
      "14912 14976\n",
      "14976 15040\n",
      "15040 15104\n",
      "15104 15168\n",
      "15168 15232\n",
      "15232 15296\n",
      "15296 15360\n",
      "15360 15424\n",
      "15424 15488\n",
      "15488 15552\n",
      "15552 15616\n",
      "15616 15680\n",
      "15680 15744\n",
      "15744 15808\n",
      "15808 15872\n",
      "15872 15936\n",
      "15936 16000\n",
      "16000 16064\n",
      "16064 16128\n",
      "16128 16192\n",
      "16192 16256\n",
      "16256 16320\n",
      "16320 16384\n",
      "16384 16448\n",
      "16448 16512\n",
      "16512 16576\n",
      "16576 16640\n",
      "16640 16704\n",
      "16704 16768\n",
      "16768 16832\n",
      "16832 16896\n",
      "16896 16960\n",
      "16960 17024\n",
      "17024 17088\n",
      "17088 17152\n",
      "17152 17216\n",
      "17216 17280\n",
      "17280 17344\n",
      "17344 17408\n",
      "17408 17472\n",
      "17472 17536\n",
      "17536 17600\n",
      "17600 17664\n",
      "17664 17728\n",
      "17728 17792\n",
      "17792 17856\n",
      "17856 17920\n",
      "17920 17984\n",
      "17984 18048\n",
      "18048 18112\n",
      "18112 18176\n",
      "18176 18240\n",
      "18240 18304\n",
      "18304 18368\n",
      "18368 18432\n",
      "18432 18496\n",
      "18496 18560\n",
      "18560 18624\n",
      "18624 18688\n",
      "18688 18752\n",
      "18752 18816\n",
      "18816 18880\n",
      "18880 18944\n",
      "18944 19008\n",
      "19008 19072\n",
      "19072 19136\n",
      "19136 19200\n",
      "19200 19264\n",
      "19264 19328\n",
      "19328 19392\n",
      "19392 19456\n",
      "19456 19520\n",
      "19520 19584\n",
      "19584 19648\n",
      "19648 19712\n",
      "19712 19776\n",
      "19776 19840\n",
      "19840 19904\n",
      "19904 19968\n",
      "19968 20032\n",
      "20032 20096\n",
      "20096 20160\n",
      "20160 20224\n",
      "20224 20288\n",
      "20288 20352\n",
      "20352 20416\n",
      "20416 20480\n",
      "20480 20544\n",
      "20544 20608\n",
      "20608 20672\n",
      "20672 20736\n",
      "20736 20800\n",
      "20800 20864\n",
      "20864 20928\n",
      "20928 20992\n",
      "20992 21056\n",
      "21056 21120\n",
      "21120 21184\n",
      "21184 21248\n",
      "21248 21312\n",
      "21312 21376\n",
      "21376 21440\n",
      "21440 21504\n",
      "21504 21568\n",
      "21568 21632\n",
      "21632 21696\n",
      "21696 21760\n",
      "21760 21824\n",
      "21824 21888\n",
      "21888 21952\n",
      "21952 22016\n",
      "22016 22080\n",
      "22080 22144\n",
      "22144 22208\n",
      "22208 22272\n",
      "22272 22336\n",
      "22336 22400\n",
      "22400 22464\n",
      "22464 22528\n",
      "22528 22592\n",
      "22592 22656\n",
      "22656 22720\n",
      "22720 22784\n",
      "22784 22848\n",
      "22848 22912\n",
      "22912 22976\n",
      "22976 23040\n",
      "23040 23104\n",
      "23104 23168\n",
      "23168 23232\n",
      "23232 23296\n",
      "23296 23360\n",
      "23360 23424\n",
      "23424 23488\n",
      "23488 23552\n",
      "23552 23616\n",
      "23616 23680\n",
      "23680 23744\n",
      "23744 23808\n",
      "23808 23872\n",
      "23872 23936\n",
      "23936 24000\n",
      "24000 24064\n",
      "24064 24128\n",
      "24128 24192\n",
      "24192 24256\n",
      "24256 24320\n",
      "24320 24384\n",
      "24384 24448\n",
      "24448 24512\n",
      "24512 24576\n",
      "24576 24640\n",
      "24640 24704\n",
      "24704 24768\n",
      "24768 24832\n",
      "24832 24896\n",
      "24896 24960\n",
      "24960 25024\n",
      "25024 25088\n",
      "25088 25152\n",
      "25152 25216\n",
      "25216 25280\n",
      "25280 25344\n",
      "25344 25408\n",
      "25408 25472\n",
      "25472 25536\n",
      "25536 25600\n",
      "25600 25664\n",
      "25664 25728\n",
      "25728 25792\n",
      "25792 25856\n",
      "25856 25920\n",
      "25920 25984\n",
      "25984 26048\n",
      "26048 26112\n",
      "26112 26176\n",
      "26176 26240\n",
      "26240 26304\n",
      "26304 26368\n",
      "26368 26432\n",
      "26432 26496\n",
      "26496 26560\n",
      "26560 26624\n",
      "26624 26688\n",
      "26688 26752\n",
      "26752 26816\n",
      "26816 26880\n",
      "26880 26944\n",
      "26944 27008\n",
      "27008 27072\n",
      "27072 27136\n",
      "27136 27200\n",
      "27200 27264\n",
      "27264 27328\n",
      "27328 27392\n",
      "27392 27456\n",
      "27456 27520\n",
      "27520 27584\n",
      "27584 27648\n",
      "27648 27712\n",
      "27712 27776\n",
      "27776 27840\n",
      "27840 27904\n",
      "27904 27968\n",
      "27968 28032\n",
      "28032 28096\n",
      "28096 28160\n",
      "28160 28224\n",
      "28224 28288\n",
      "28288 28352\n",
      "28352 28416\n",
      "28416 28480\n",
      "28480 28544\n",
      "28544 28608\n",
      "28608 28672\n",
      "28672 28736\n",
      "28736 28800\n",
      "28800 28864\n",
      "28864 28928\n",
      "28928 28992\n",
      "28992 29056\n",
      "29056 29120\n",
      "29120 29184\n",
      "29184 29248\n",
      "29248 29312\n",
      "29312 29376\n",
      "29376 29440\n",
      "29440 29504\n",
      "29504 29568\n",
      "29568 29632\n",
      "29632 29696\n",
      "29696 29760\n",
      "29760 29824\n",
      "29824 29888\n",
      "29888 29952\n",
      "29952 30016\n",
      "30016 30080\n",
      "30080 30144\n",
      "30144 30208\n",
      "30208 30272\n",
      "30272 30336\n",
      "30336 30400\n",
      "30400 30464\n",
      "30464 30528\n",
      "30528 30592\n",
      "30592 30656\n",
      "30656 30720\n",
      "30720 30784\n",
      "30784 30848\n",
      "30848 30912\n",
      "30912 30976\n",
      "30976 31040\n",
      "31040 31104\n",
      "31104 31168\n",
      "31168 31232\n",
      "31232 31296\n",
      "31296 31360\n",
      "31360 31424\n",
      "31424 31488\n",
      "31488 31552\n",
      "31552 31616\n",
      "31616 31680\n",
      "31680 31744\n",
      "31744 31808\n",
      "31808 31872\n",
      "31872 31936\n",
      "31936 32000\n",
      "32000 32064\n",
      "32064 32128\n",
      "32128 32192\n",
      "32192 32256\n",
      "32256 32320\n",
      "32320 32384\n",
      "32384 32448\n",
      "32448 32512\n",
      "32512 32576\n",
      "32576 32640\n",
      "32640 32704\n",
      "32704 32768\n",
      "32768 32832\n",
      "32832 32896\n",
      "32896 32960\n",
      "32960 33024\n",
      "33024 33088\n",
      "33088 33152\n",
      "33152 33216\n",
      "33216 33280\n",
      "33280 33344\n",
      "33344 33408\n",
      "33408 33472\n",
      "33472 33536\n",
      "33536 33600\n",
      "33600 33664\n",
      "33664 33728\n",
      "33728 33792\n",
      "33792 33856\n",
      "33856 33920\n",
      "33920 33984\n",
      "33984 34048\n",
      "34048 34112\n",
      "34112 34176\n",
      "34176 34240\n",
      "34240 34304\n",
      "34304 34368\n",
      "34368 34432\n",
      "34432 34496\n",
      "34496 34560\n",
      "34560 34624\n",
      "34624 34688\n",
      "34688 34752\n",
      "34752 34816\n",
      "34816 34880\n",
      "34880 34944\n",
      "34944 35008\n",
      "35008 35072\n",
      "35072 35136\n",
      "35136 35200\n",
      "35200 35264\n",
      "35264 35328\n",
      "35328 35392\n",
      "35392 35456\n",
      "35456 35520\n",
      "35520 35584\n",
      "35584 35648\n",
      "35648 35712\n",
      "35712 35776\n",
      "35776 35840\n",
      "35840 35904\n",
      "35904 35968\n",
      "35968 36032\n",
      "36032 36096\n",
      "36096 36160\n",
      "36160 36224\n",
      "36224 36288\n",
      "36288 36352\n",
      "36352 36416\n",
      "36416 36480\n",
      "36480 36544\n",
      "36544 36608\n",
      "36608 36672\n",
      "36672 36736\n",
      "36736 36800\n",
      "36800 36864\n",
      "36864 36928\n",
      "36928 36992\n",
      "36992 37056\n",
      "37056 37120\n",
      "37120 37184\n",
      "37184 37248\n",
      "37248 37312\n",
      "37312 37376\n",
      "37376 37440\n",
      "37440 37504\n",
      "37504 37568\n",
      "37568 37632\n",
      "37632 37696\n",
      "37696 37760\n",
      "37760 37824\n",
      "37824 37888\n",
      "37888 37952\n",
      "37952 38016\n",
      "38016 38080\n",
      "38080 38144\n",
      "38144 38208\n",
      "38208 38272\n",
      "38272 38336\n",
      "38336 38400\n",
      "38400 38464\n",
      "38464 38528\n",
      "38528 38592\n",
      "38592 38656\n",
      "38656 38720\n",
      "38720 38784\n",
      "38784 38848\n",
      "38848 38912\n",
      "38912 38976\n",
      "38976 39040\n",
      "39040 39104\n",
      "39104 39168\n",
      "39168 39232\n",
      "39232 39296\n",
      "39296 39360\n",
      "39360 39424\n",
      "39424 39488\n",
      "39488 39552\n",
      "39552 39616\n",
      "39616 39680\n",
      "39680 39744\n",
      "39744 39808\n",
      "39808 39872\n",
      "39872 39936\n",
      "39936 40000\n",
      "40000 40064\n",
      "40064 40128\n",
      "40128 40192\n",
      "40192 40256\n",
      "40256 40320\n",
      "40320 40384\n",
      "40384 40448\n",
      "40448 40512\n",
      "40512 40576\n",
      "40576 40640\n",
      "40640 40704\n",
      "40704 40768\n",
      "40768 40832\n",
      "40832 40896\n",
      "40896 40960\n",
      "40960 41024\n",
      "41024 41088\n",
      "41088 41152\n",
      "41152 41216\n",
      "41216 41280\n",
      "41280 41344\n",
      "41344 41408\n",
      "41408 41472\n",
      "41472 41536\n",
      "41536 41600\n",
      "41600 41664\n",
      "41664 41728\n",
      "41728 41792\n",
      "41792 41856\n",
      "41856 41920\n",
      "41920 41984\n",
      "41984 42048\n",
      "42048 42112\n",
      "42112 42176\n",
      "42176 42240\n",
      "42240 42304\n",
      "42304 42368\n",
      "42368 42432\n",
      "42432 42496\n",
      "42496 42560\n",
      "42560 42624\n",
      "42624 42688\n",
      "42688 42752\n",
      "42752 42816\n",
      "42816 42880\n",
      "42880 42944\n",
      "42944 43008\n",
      "43008 43072\n",
      "43072 43136\n",
      "43136 43200\n",
      "43200 43264\n",
      "43264 43328\n",
      "43328 43392\n",
      "43392 43456\n",
      "43456 43520\n",
      "43520 43584\n",
      "43584 43648\n",
      "43648 43712\n",
      "43712 43776\n",
      "43776 43840\n",
      "43840 43904\n",
      "43904 43968\n",
      "43968 44032\n",
      "44032 44096\n",
      "44096 44160\n",
      "44160 44224\n",
      "44224 44288\n",
      "44288 44352\n",
      "44352 44416\n",
      "44416 44480\n",
      "44480 44544\n",
      "44544 44608\n",
      "44608 44672\n",
      "44672 44736\n",
      "44736 44800\n",
      "44800 44864\n",
      "44864 44928\n",
      "44928 44992\n",
      "44992 45056\n",
      "45056 45120\n",
      "45120 45184\n",
      "45184 45248\n",
      "45248 45312\n",
      "45312 45376\n",
      "45376 45440\n",
      "45440 45504\n",
      "45504 45568\n",
      "45568 45632\n",
      "45632 45696\n",
      "45696 45760\n",
      "45760 45824\n",
      "45824 45888\n",
      "45888 45952\n",
      "45952 46016\n",
      "46016 46080\n",
      "46080 46144\n",
      "46144 46208\n",
      "46208 46272\n",
      "46272 46336\n",
      "46336 46400\n",
      "46400 46464\n",
      "46464 46528\n",
      "46528 46592\n",
      "46592 46656\n",
      "46656 46720\n",
      "46720 46784\n",
      "46784 46848\n",
      "46848 46912\n",
      "46912 46976\n",
      "46976 47040\n",
      "47040 47104\n",
      "47104 47168\n",
      "47168 47232\n",
      "47232 47296\n",
      "47296 47360\n",
      "47360 47424\n",
      "47424 47488\n",
      "47488 47552\n",
      "47552 47616\n",
      "47616 47680\n",
      "47680 47744\n",
      "47744 47808\n",
      "47808 47872\n",
      "47872 47936\n",
      "47936 48000\n",
      "48000 48064\n",
      "48064 48128\n",
      "48128 48192\n",
      "48192 48256\n",
      "48256 48320\n",
      "48320 48384\n",
      "48384 48448\n",
      "48448 48512\n",
      "48512 48576\n",
      "48576 48640\n",
      "48640 48704\n",
      "48704 48768\n",
      "48768 48832\n",
      "48832 48896\n",
      "48896 48960\n",
      "48960 49024\n",
      "49024 49088\n",
      "49088 49152\n",
      "49152 49216\n",
      "49216 49280\n",
      "49280 49344\n",
      "49344 49408\n",
      "49408 49472\n",
      "49472 49536\n",
      "49536 49600\n",
      "49600 49664\n",
      "49664 49728\n",
      "49728 49792\n",
      "49792 49856\n",
      "49856 49920\n",
      "49920 49984\n",
      "49984 50048\n",
      "0 64\n",
      "64 128\n",
      "128 192\n",
      "192 256\n",
      "256 320\n",
      "320 384\n",
      "384 448\n",
      "448 512\n",
      "512 576\n",
      "576 640\n",
      "640 704\n",
      "704 768\n",
      "768 832\n",
      "832 896\n",
      "896 960\n",
      "960 1024\n",
      "1024 1088\n",
      "1088 1152\n",
      "1152 1216\n",
      "1216 1280\n",
      "1280 1344\n",
      "1344 1408\n",
      "1408 1472\n",
      "1472 1536\n",
      "1536 1600\n",
      "1600 1664\n",
      "1664 1728\n",
      "1728 1792\n",
      "1792 1856\n",
      "1856 1920\n",
      "1920 1984\n",
      "1984 2048\n",
      "2048 2112\n",
      "2112 2176\n",
      "2176 2240\n",
      "2240 2304\n",
      "2304 2368\n",
      "2368 2432\n",
      "2432 2496\n",
      "2496 2560\n",
      "2560 2624\n",
      "2624 2688\n",
      "2688 2752\n",
      "2752 2816\n",
      "2816 2880\n",
      "2880 2944\n",
      "2944 3008\n",
      "3008 3072\n",
      "3072 3136\n",
      "3136 3200\n",
      "3200 3264\n",
      "3264 3328\n",
      "3328 3392\n",
      "3392 3456\n",
      "3456 3520\n",
      "3520 3584\n",
      "3584 3648\n",
      "3648 3712\n",
      "3712 3776\n",
      "3776 3840\n",
      "3840 3904\n",
      "3904 3968\n",
      "3968 4032\n",
      "4032 4096\n",
      "4096 4160\n",
      "4160 4224\n",
      "4224 4288\n",
      "4288 4352\n",
      "4352 4416\n",
      "4416 4480\n",
      "4480 4544\n",
      "4544 4608\n",
      "4608 4672\n",
      "4672 4736\n",
      "4736 4800\n",
      "4800 4864\n",
      "4864 4928\n",
      "4928 4992\n",
      "4992 5056\n",
      "5056 5120\n",
      "5120 5184\n",
      "5184 5248\n",
      "5248 5312\n",
      "5312 5376\n",
      "5376 5440\n",
      "5440 5504\n",
      "5504 5568\n",
      "5568 5632\n",
      "5632 5696\n",
      "5696 5760\n",
      "5760 5824\n",
      "5824 5888\n",
      "5888 5952\n",
      "5952 6016\n",
      "6016 6080\n",
      "6080 6144\n",
      "6144 6208\n",
      "6208 6272\n",
      "6272 6336\n",
      "6336 6400\n",
      "6400 6464\n",
      "6464 6528\n",
      "6528 6592\n",
      "6592 6656\n",
      "6656 6720\n",
      "6720 6784\n",
      "6784 6848\n",
      "6848 6912\n",
      "6912 6976\n",
      "6976 7040\n",
      "7040 7104\n",
      "7104 7168\n",
      "7168 7232\n",
      "7232 7296\n",
      "7296 7360\n",
      "7360 7424\n",
      "7424 7488\n",
      "7488 7552\n",
      "7552 7616\n",
      "7616 7680\n",
      "7680 7744\n",
      "7744 7808\n",
      "7808 7872\n",
      "7872 7936\n",
      "7936 8000\n",
      "8000 8064\n",
      "8064 8128\n",
      "8128 8192\n",
      "8192 8256\n",
      "8256 8320\n",
      "8320 8384\n",
      "8384 8448\n",
      "8448 8512\n",
      "8512 8576\n",
      "8576 8640\n",
      "8640 8704\n",
      "8704 8768\n",
      "8768 8832\n",
      "8832 8896\n",
      "8896 8960\n",
      "8960 9024\n",
      "9024 9088\n",
      "9088 9152\n",
      "9152 9216\n",
      "9216 9280\n",
      "9280 9344\n",
      "9344 9408\n",
      "9408 9472\n",
      "9472 9536\n",
      "9536 9600\n",
      "9600 9664\n",
      "9664 9728\n",
      "9728 9792\n",
      "9792 9856\n",
      "9856 9920\n",
      "9920 9984\n",
      "9984 10048\n",
      "10048 10112\n",
      "10112 10176\n",
      "10176 10240\n",
      "10240 10304\n",
      "10304 10368\n",
      "10368 10432\n",
      "10432 10496\n",
      "10496 10560\n",
      "10560 10624\n",
      "10624 10688\n",
      "10688 10752\n",
      "10752 10816\n",
      "10816 10880\n",
      "10880 10944\n",
      "10944 11008\n",
      "11008 11072\n",
      "11072 11136\n",
      "11136 11200\n",
      "11200 11264\n",
      "11264 11328\n",
      "11328 11392\n",
      "11392 11456\n",
      "11456 11520\n",
      "11520 11584\n",
      "11584 11648\n",
      "11648 11712\n",
      "11712 11776\n",
      "11776 11840\n",
      "11840 11904\n",
      "11904 11968\n",
      "11968 12032\n",
      "12032 12096\n",
      "12096 12160\n",
      "12160 12224\n",
      "12224 12288\n",
      "12288 12352\n",
      "12352 12416\n",
      "12416 12480\n",
      "12480 12544\n",
      "12544 12608\n",
      "12608 12672\n",
      "12672 12736\n",
      "12736 12800\n",
      "12800 12864\n",
      "12864 12928\n",
      "12928 12992\n",
      "12992 13056\n",
      "13056 13120\n",
      "13120 13184\n",
      "13184 13248\n",
      "13248 13312\n",
      "13312 13376\n",
      "13376 13440\n",
      "13440 13504\n",
      "13504 13568\n",
      "13568 13632\n",
      "13632 13696\n",
      "13696 13760\n",
      "13760 13824\n",
      "13824 13888\n",
      "13888 13952\n",
      "13952 14016\n",
      "14016 14080\n",
      "14080 14144\n",
      "14144 14208\n",
      "14208 14272\n",
      "14272 14336\n",
      "14336 14400\n",
      "14400 14464\n",
      "14464 14528\n",
      "14528 14592\n",
      "14592 14656\n",
      "14656 14720\n",
      "14720 14784\n",
      "14784 14848\n",
      "14848 14912\n",
      "14912 14976\n",
      "14976 15040\n",
      "15040 15104\n",
      "15104 15168\n",
      "15168 15232\n",
      "15232 15296\n",
      "15296 15360\n",
      "15360 15424\n",
      "15424 15488\n",
      "15488 15552\n",
      "15552 15616\n",
      "15616 15680\n",
      "15680 15744\n",
      "15744 15808\n",
      "15808 15872\n",
      "15872 15936\n",
      "15936 16000\n",
      "16000 16064\n",
      "16064 16128\n",
      "16128 16192\n",
      "16192 16256\n",
      "16256 16320\n",
      "16320 16384\n",
      "16384 16448\n",
      "16448 16512\n",
      "16512 16576\n",
      "16576 16640\n",
      "16640 16704\n",
      "16704 16768\n",
      "16768 16832\n",
      "16832 16896\n",
      "16896 16960\n",
      "16960 17024\n",
      "17024 17088\n",
      "17088 17152\n",
      "17152 17216\n",
      "17216 17280\n",
      "17280 17344\n",
      "17344 17408\n",
      "17408 17472\n",
      "17472 17536\n",
      "17536 17600\n",
      "17600 17664\n",
      "17664 17728\n",
      "17728 17792\n",
      "17792 17856\n",
      "17856 17920\n",
      "17920 17984\n",
      "17984 18048\n",
      "18048 18112\n",
      "18112 18176\n",
      "18176 18240\n",
      "18240 18304\n",
      "18304 18368\n",
      "18368 18432\n",
      "18432 18496\n",
      "18496 18560\n",
      "18560 18624\n",
      "18624 18688\n",
      "18688 18752\n",
      "18752 18816\n",
      "18816 18880\n",
      "18880 18944\n",
      "18944 19008\n",
      "19008 19072\n",
      "19072 19136\n",
      "19136 19200\n",
      "19200 19264\n",
      "19264 19328\n",
      "19328 19392\n",
      "19392 19456\n",
      "19456 19520\n",
      "19520 19584\n",
      "19584 19648\n",
      "19648 19712\n",
      "19712 19776\n",
      "19776 19840\n",
      "19840 19904\n",
      "19904 19968\n",
      "19968 20032\n",
      "20032 20096\n",
      "20096 20160\n",
      "20160 20224\n",
      "20224 20288\n",
      "20288 20352\n",
      "20352 20416\n",
      "20416 20480\n",
      "20480 20544\n",
      "20544 20608\n",
      "20608 20672\n",
      "20672 20736\n",
      "20736 20800\n",
      "20800 20864\n",
      "20864 20928\n",
      "20928 20992\n",
      "20992 21056\n",
      "21056 21120\n",
      "21120 21184\n",
      "21184 21248\n",
      "21248 21312\n",
      "21312 21376\n",
      "21376 21440\n",
      "21440 21504\n",
      "21504 21568\n",
      "21568 21632\n",
      "21632 21696\n",
      "21696 21760\n",
      "21760 21824\n",
      "21824 21888\n",
      "21888 21952\n",
      "21952 22016\n",
      "22016 22080\n",
      "22080 22144\n",
      "22144 22208\n",
      "22208 22272\n",
      "22272 22336\n",
      "22336 22400\n",
      "22400 22464\n",
      "22464 22528\n",
      "22528 22592\n",
      "22592 22656\n",
      "22656 22720\n",
      "22720 22784\n",
      "22784 22848\n",
      "22848 22912\n",
      "22912 22976\n",
      "22976 23040\n",
      "23040 23104\n",
      "23104 23168\n",
      "23168 23232\n",
      "23232 23296\n",
      "23296 23360\n",
      "23360 23424\n",
      "23424 23488\n",
      "23488 23552\n",
      "23552 23616\n",
      "23616 23680\n",
      "23680 23744\n",
      "23744 23808\n",
      "23808 23872\n",
      "23872 23936\n",
      "23936 24000\n",
      "24000 24064\n",
      "24064 24128\n",
      "24128 24192\n",
      "24192 24256\n",
      "24256 24320\n",
      "24320 24384\n",
      "24384 24448\n",
      "24448 24512\n",
      "24512 24576\n",
      "24576 24640\n",
      "24640 24704\n",
      "24704 24768\n",
      "24768 24832\n",
      "24832 24896\n",
      "24896 24960\n",
      "24960 25024\n",
      "25024 25088\n",
      "25088 25152\n",
      "25152 25216\n",
      "25216 25280\n",
      "25280 25344\n",
      "25344 25408\n",
      "25408 25472\n",
      "25472 25536\n",
      "25536 25600\n",
      "25600 25664\n",
      "25664 25728\n",
      "25728 25792\n",
      "25792 25856\n",
      "25856 25920\n",
      "25920 25984\n",
      "25984 26048\n",
      "26048 26112\n",
      "26112 26176\n",
      "26176 26240\n",
      "26240 26304\n",
      "26304 26368\n",
      "26368 26432\n",
      "26432 26496\n",
      "26496 26560\n",
      "26560 26624\n",
      "26624 26688\n",
      "26688 26752\n",
      "26752 26816\n",
      "26816 26880\n",
      "26880 26944\n",
      "26944 27008\n",
      "27008 27072\n",
      "27072 27136\n",
      "27136 27200\n",
      "27200 27264\n",
      "27264 27328\n",
      "27328 27392\n",
      "27392 27456\n",
      "27456 27520\n",
      "27520 27584\n",
      "27584 27648\n",
      "27648 27712\n",
      "27712 27776\n",
      "27776 27840\n",
      "27840 27904\n",
      "27904 27968\n",
      "27968 28032\n",
      "28032 28096\n",
      "28096 28160\n",
      "28160 28224\n",
      "28224 28288\n",
      "28288 28352\n",
      "28352 28416\n",
      "28416 28480\n",
      "28480 28544\n",
      "28544 28608\n",
      "28608 28672\n",
      "28672 28736\n",
      "28736 28800\n",
      "28800 28864\n",
      "28864 28928\n",
      "28928 28992\n",
      "28992 29056\n",
      "29056 29120\n",
      "29120 29184\n",
      "29184 29248\n",
      "29248 29312\n",
      "29312 29376\n",
      "29376 29440\n",
      "29440 29504\n",
      "29504 29568\n",
      "29568 29632\n",
      "29632 29696\n",
      "29696 29760\n",
      "29760 29824\n",
      "29824 29888\n",
      "29888 29952\n",
      "29952 30016\n",
      "30016 30080\n",
      "30080 30144\n",
      "30144 30208\n",
      "30208 30272\n",
      "30272 30336\n",
      "30336 30400\n",
      "30400 30464\n",
      "30464 30528\n",
      "30528 30592\n",
      "30592 30656\n",
      "30656 30720\n",
      "30720 30784\n",
      "30784 30848\n",
      "30848 30912\n",
      "30912 30976\n",
      "30976 31040\n",
      "31040 31104\n",
      "31104 31168\n",
      "31168 31232\n",
      "31232 31296\n",
      "31296 31360\n",
      "31360 31424\n",
      "31424 31488\n",
      "31488 31552\n",
      "31552 31616\n",
      "31616 31680\n",
      "31680 31744\n",
      "31744 31808\n",
      "31808 31872\n",
      "31872 31936\n",
      "31936 32000\n",
      "32000 32064\n",
      "32064 32128\n",
      "32128 32192\n",
      "32192 32256\n",
      "32256 32320\n",
      "32320 32384\n",
      "32384 32448\n",
      "32448 32512\n",
      "32512 32576\n",
      "32576 32640\n",
      "32640 32704\n",
      "32704 32768\n",
      "32768 32832\n",
      "32832 32896\n",
      "32896 32960\n",
      "32960 33024\n",
      "33024 33088\n",
      "33088 33152\n",
      "33152 33216\n",
      "33216 33280\n",
      "33280 33344\n",
      "33344 33408\n",
      "33408 33472\n",
      "33472 33536\n",
      "33536 33600\n",
      "33600 33664\n",
      "33664 33728\n",
      "33728 33792\n",
      "33792 33856\n",
      "33856 33920\n",
      "33920 33984\n",
      "33984 34048\n",
      "34048 34112\n",
      "34112 34176\n",
      "34176 34240\n",
      "34240 34304\n",
      "34304 34368\n",
      "34368 34432\n",
      "34432 34496\n",
      "34496 34560\n",
      "34560 34624\n",
      "34624 34688\n",
      "34688 34752\n",
      "34752 34816\n",
      "34816 34880\n",
      "34880 34944\n",
      "34944 35008\n",
      "35008 35072\n",
      "35072 35136\n",
      "35136 35200\n",
      "35200 35264\n",
      "35264 35328\n",
      "35328 35392\n",
      "35392 35456\n",
      "35456 35520\n",
      "35520 35584\n",
      "35584 35648\n",
      "35648 35712\n",
      "35712 35776\n",
      "35776 35840\n",
      "35840 35904\n",
      "35904 35968\n",
      "35968 36032\n",
      "36032 36096\n",
      "36096 36160\n",
      "36160 36224\n",
      "36224 36288\n",
      "36288 36352\n",
      "36352 36416\n",
      "36416 36480\n",
      "36480 36544\n",
      "36544 36608\n",
      "36608 36672\n",
      "36672 36736\n",
      "36736 36800\n",
      "36800 36864\n",
      "36864 36928\n",
      "36928 36992\n",
      "36992 37056\n",
      "37056 37120\n",
      "37120 37184\n",
      "37184 37248\n",
      "37248 37312\n",
      "37312 37376\n",
      "37376 37440\n",
      "37440 37504\n",
      "37504 37568\n",
      "37568 37632\n",
      "37632 37696\n",
      "37696 37760\n",
      "37760 37824\n",
      "37824 37888\n",
      "37888 37952\n",
      "37952 38016\n",
      "38016 38080\n",
      "38080 38144\n",
      "38144 38208\n",
      "38208 38272\n",
      "38272 38336\n",
      "38336 38400\n",
      "38400 38464\n",
      "38464 38528\n",
      "38528 38592\n",
      "38592 38656\n",
      "38656 38720\n",
      "38720 38784\n",
      "38784 38848\n",
      "38848 38912\n",
      "38912 38976\n",
      "38976 39040\n",
      "39040 39104\n",
      "39104 39168\n",
      "39168 39232\n",
      "39232 39296\n",
      "39296 39360\n",
      "39360 39424\n",
      "39424 39488\n",
      "39488 39552\n",
      "39552 39616\n",
      "39616 39680\n",
      "39680 39744\n",
      "39744 39808\n",
      "39808 39872\n",
      "39872 39936\n",
      "39936 40000\n",
      "40000 40064\n",
      "40064 40128\n",
      "40128 40192\n",
      "40192 40256\n",
      "40256 40320\n",
      "40320 40384\n",
      "40384 40448\n",
      "40448 40512\n",
      "40512 40576\n",
      "40576 40640\n",
      "40640 40704\n",
      "40704 40768\n",
      "40768 40832\n",
      "40832 40896\n",
      "40896 40960\n",
      "40960 41024\n",
      "41024 41088\n",
      "41088 41152\n",
      "41152 41216\n",
      "41216 41280\n",
      "41280 41344\n",
      "41344 41408\n",
      "41408 41472\n",
      "41472 41536\n",
      "41536 41600\n",
      "41600 41664\n",
      "41664 41728\n",
      "41728 41792\n",
      "41792 41856\n",
      "41856 41920\n",
      "41920 41984\n",
      "41984 42048\n",
      "42048 42112\n",
      "42112 42176\n",
      "42176 42240\n",
      "42240 42304\n",
      "42304 42368\n",
      "42368 42432\n",
      "42432 42496\n",
      "42496 42560\n",
      "42560 42624\n",
      "42624 42688\n",
      "42688 42752\n",
      "42752 42816\n",
      "42816 42880\n",
      "42880 42944\n",
      "42944 43008\n",
      "43008 43072\n",
      "43072 43136\n",
      "43136 43200\n",
      "43200 43264\n",
      "43264 43328\n",
      "43328 43392\n",
      "43392 43456\n",
      "43456 43520\n",
      "43520 43584\n",
      "43584 43648\n",
      "43648 43712\n",
      "43712 43776\n",
      "43776 43840\n",
      "43840 43904\n",
      "43904 43968\n",
      "43968 44032\n",
      "44032 44096\n",
      "44096 44160\n",
      "44160 44224\n",
      "44224 44288\n",
      "44288 44352\n",
      "44352 44416\n",
      "44416 44480\n",
      "44480 44544\n",
      "44544 44608\n",
      "44608 44672\n",
      "44672 44736\n",
      "44736 44800\n",
      "44800 44864\n",
      "44864 44928\n",
      "44928 44992\n",
      "44992 45056\n",
      "45056 45120\n",
      "45120 45184\n",
      "45184 45248\n",
      "45248 45312\n",
      "45312 45376\n",
      "45376 45440\n",
      "45440 45504\n",
      "45504 45568\n",
      "45568 45632\n",
      "45632 45696\n",
      "45696 45760\n",
      "45760 45824\n",
      "45824 45888\n",
      "45888 45952\n",
      "45952 46016\n",
      "46016 46080\n",
      "46080 46144\n",
      "46144 46208\n",
      "46208 46272\n",
      "46272 46336\n",
      "46336 46400\n",
      "46400 46464\n",
      "46464 46528\n",
      "46528 46592\n",
      "46592 46656\n",
      "46656 46720\n",
      "46720 46784\n",
      "46784 46848\n",
      "46848 46912\n",
      "46912 46976\n",
      "46976 47040\n",
      "47040 47104\n",
      "47104 47168\n",
      "47168 47232\n",
      "47232 47296\n",
      "47296 47360\n",
      "47360 47424\n",
      "47424 47488\n",
      "47488 47552\n",
      "47552 47616\n",
      "47616 47680\n",
      "47680 47744\n",
      "47744 47808\n",
      "47808 47872\n",
      "47872 47936\n",
      "47936 48000\n",
      "48000 48064\n",
      "48064 48128\n",
      "48128 48192\n",
      "48192 48256\n",
      "48256 48320\n",
      "48320 48384\n",
      "48384 48448\n",
      "48448 48512\n",
      "48512 48576\n",
      "48576 48640\n",
      "48640 48704\n",
      "48704 48768\n",
      "48768 48832\n",
      "48832 48896\n",
      "48896 48960\n",
      "48960 49024\n",
      "49024 49088\n",
      "49088 49152\n",
      "49152 49216\n",
      "49216 49280\n",
      "49280 49344\n",
      "49344 49408\n",
      "49408 49472\n",
      "49472 49536\n",
      "49536 49600\n",
      "49600 49664\n",
      "49664 49728\n",
      "49728 49792\n",
      "49792 49856\n",
      "49856 49920\n",
      "49920 49984\n",
      "49984 50048\n"
     ]
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            print(start_i, end_i)\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check that our loss has gone down:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0791, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using nn.Linear\n-------------------------\n\nWe continue to refactor our code.  Instead of manually defining and\ninitializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\nself.weights + self.bias``, we will instead use the Pytorch class\n`nn.Linear <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ for a\nlinear layer, which does all that for us. Pytorch has many types of\npredefined layers that can greatly simplify our code, and often makes it\nfaster too.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our model and calculate the loss in the same way as before:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3263, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\nprint(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still able to use our same ``fit`` method as before.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0806, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n\nprint(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using optim\n------------------------------\n\nPytorch also has a package with various optimization algorithms, ``torch.optim``.\nWe can use the ``step`` method from our optimizer to take a forward step, instead\nof manually updating each parameter.\n\nThis will let us replace our previous manually coded optimization step:\n::\n  with torch.no_grad():\n      for p in model.parameters(): p -= p.grad * lr\n      model.zero_grad()\n\nand instead use just:\n::\n  opt.step()\n  opt.zero_grad()\n\n(``optim.zero_grad()`` resets the gradient to 0 and we need to call it before\ncomputing the gradient for the next minibatch.)\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a little function to create our model and optimizer so we\ncan reuse it in the future.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3102, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)\n\nmodel, opt = get_model()\nprint(loss_func(model(xb), yb))\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using Dataset\n------------------------------\n\nPyTorch has an abstract Dataset class.  A Dataset can be anything that has\na ``__len__`` function (called by Python's standard ``len`` function) and\na ``__getitem__`` function as a way of indexing into it.\n`This tutorial <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_\nwalks through a nice example of creating a custom ``FacialLandmarkDataset`` class\nas a subclass of ``Dataset``.\n\nPyTorch's `TensorDataset <https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset>`_\nis a Dataset wrapping tensors. By defining a length and way of indexing,\nthis also gives us a way to iterate, index, and slice along the first\ndimension of a tensor. This will make it easier to access both the\nindependent and dependent variables in the same line as we train.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``,\nwhich will be easier to iterate over and slice.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we had to iterate through minibatches of x and y values separately:\n::\n    xb = x_train[start_i:end_i]\n    yb = y_train[start_i:end_i]\n\n\nNow, we can do these two steps together:\n::\n    xb,yb = train_ds[i*bs : i*bs+bs]\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0811, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        xb, yb = train_ds[i * bs: i * bs + bs]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using DataLoader\n------------------------------\n\nPytorch's ``DataLoader`` is responsible for managing batches. You can\ncreate a ``DataLoader`` from any ``Dataset``. ``DataLoader`` makes it easier\nto iterate over batches. Rather than having to use ``train_ds[i*bs : i*bs+bs]``,\nthe DataLoader gives us each minibatch automatically.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n::\n      for i in range((n-1)//bs + 1):\n          xb,yb = train_ds[i*bs : i*bs+bs]\n          pred = model(xb)\n\nNow, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:\n::\n      for xb,yb in train_dl:\n          pred = model(xb)\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0823, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n\nfor epoch in range(epochs):\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``,\nour training loop is now dramatically smaller and easier to understand. Let's\nnow try to add the basic features necessary to create effective models in practice.\n\nAdd validation\n-----------------------\n\nIn section 1, we were just trying to get a reasonable training loop set up for\nuse on our training data.  In reality, you **always** should also have\na `validation set <https://www.fast.ai/2017/11/13/validation-sets/>`_, in order\nto identify if you are overfitting.\n\nShuffling the training data is\n`important <https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks>`_\nto prevent correlation between batches and overfitting. On the other hand, the\nvalidation loss will be identical whether we shuffle the validation set or not.\nSince shuffling takes extra time, it makes no sense to shuffle the validation data.\n\nWe'll use a batch size for the validation set that is twice as large as\nthat for the training set. This is because the validation set does not\nneed backpropagation and thus takes less memory (it doesn't need to\nstore the gradients). We take advantage of this to use a larger batch\nsize and compute the loss more quickly.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n\nvalid_ds = TensorDataset(x_valid, y_valid)\nvalid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate and print the validation loss at the end of each epoch.\n\n(Note that we always call ``model.train()`` before training, and ``model.eval()``\nbefore inference, because these are used by layers such as ``nn.BatchNorm2d``\nand ``nn.Dropout`` to ensure appropriate behaviour for these different phases.)\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, opt = get_model()\n\nfor epoch in range(epochs):\n    model.train()\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    model.eval()\n    with torch.no_grad():\n        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n\n    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fit() and get_data()\n----------------------------------\n\nWe'll now do a little refactoring of our own. Since we go through a similar\nprocess twice of calculating the loss for both the training set and the\nvalidation set, let's make that into its own function, ``loss_batch``, which\ncomputes the loss for one batch.\n\nWe pass an optimizer in for the training set, and use it to perform\nbackprop.  For the validation set, we don't pass an optimizer, so the\nmethod doesn't perform backprop.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``fit`` runs the necessary operations to train our model and compute the\ntraining and validation losses for each epoch.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_data`` returns dataloaders for the training and validation sets.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the\nmodel can be run in 3 lines of code:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\nmodel, opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these basic 3 lines of code to train a wide variety of models.\nLet's see if we can use them to train a convolutional neural network (CNN)!\n\nSwitch to CNN\n-------------\n\nWe are now going to build our neural network with three convolutional layers.\nBecause none of the functions in the previous section assume anything about\nthe model form, we'll be able to use them to train a CNN without any modification.\n\nWe will use Pytorch's predefined\n`Conv2d <https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ class\nas our convolutional layer. We define a CNN with 3 convolutional layers.\nEach convolution is followed by a ReLU.  At the end, we perform an\naverage pooling.  (Note that ``view`` is PyTorch's version of numpy's\n``reshape``)\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nlr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_ is a variation on\nstochastic gradient descent that takes previous updates into account as well\nand generally leads to faster training.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Mnist_CNN()\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential\n------------------------\n\n``torch.nn`` has another handy class we can use to simplify our code:\n`Sequential <https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential>`_ .\nA ``Sequential`` object runs each of the modules contained within it, in a\nsequential manner. This is a simpler way of writing our neural network.\n\nTo take advantage of this, we need to be able to easily define a\n**custom layer** from a given function.  For instance, PyTorch doesn't\nhave a `view` layer, and we need to create one for our network. ``Lambda``\nwill create a layer that we can then use when defining a network with\n``Sequential``.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model created with ``Sequential`` is simply:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n    Lambda(preprocess),\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AvgPool2d(4),\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping DataLoader\n-----------------------------\n\nOur CNN is fairly concise, but it only works with MNIST, because:\n - It assumes the input is a 28\\*28 long vector\n - It assumes that the final CNN grid size is 4\\*4 (since that's the average pooling kernel size we used)\n\nLet's get rid of these two assumptions, so our model works with any 2d\nsingle channel image. First, we can remove the initial Lambda layer by\nmoving the data preprocessing into a generator:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\nallows us to define the size of the *output* tensor we want, rather than\nthe *input* tensor we have. As a result, our model will work with any\nsize input.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1),\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your GPU\n---------------\n\nIf you're lucky enough to have access to a CUDA-capable GPU (you can\nrent one for about $0.50/hour from most cloud providers) you can\nuse it to speed up your code. First check that your GPU is working in\nPytorch:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create a device object for it:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\n    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update ``preprocess`` to move batches to the GPU:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can move our model to the GPU.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.to(dev)\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find it runs faster now:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing thoughts\n-----------------\n\nWe now have a general data pipeline and training loop which you can use for\ntraining many types of models using Pytorch. To see how simple training a model\ncan now be, take a look at the `mnist_sample` sample notebook.\n\nOf course, there are many things you'll want to add, such as data augmentation,\nhyperparameter tuning, monitoring training, transfer learning, and so forth.\nThese features are available in the fastai library, which has been developed\nusing the same design approach shown in this tutorial, providing a natural\nnext step for practitioners looking to take their models further.\n\nWe promised at the start of this tutorial we'd explain through example each of\n``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's summarize\nwhat we've seen:\n\n - **torch.nn**\n\n   + ``Module``: creates a callable which behaves like a function, but can also\n     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n     contains and can zero all their gradients, loop through them for weight updates, etc.\n   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n     which contains activation functions, loss functions, etc, as well as non-stateful\n     versions of layers such as convolutional and linear layers.\n - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n   of ``Parameter`` during the backward step\n - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n   including classes provided with Pytorch such as ``TensorDataset``\n - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}